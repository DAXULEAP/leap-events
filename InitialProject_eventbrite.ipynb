{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e07aa3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Import Libraries and Initialize Parameters\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import threading\n",
    "import queue\n",
    "import pytz\n",
    "\n",
    "# Compile a regex to extract the event ID from Eventbrite URLs\n",
    "EVENT_ID_RE = re.compile(r'/e/.+-([0-9]+)(?=\\D|$)')\n",
    "\n",
    "# Record the start time to measure total runtime\n",
    "start_time = time.time()\n",
    "\n",
    "# List of keywords to search for\n",
    "keywords = [\n",
    "    \"career planning\",\n",
    "    \"career development\",\n",
    "    \"professional development\",\n",
    "    \"leadership\",\n",
    "    \"job fair\",\n",
    "    \"career fair\",\n",
    "    \"networking\"\n",
    "]\n",
    "\n",
    "# Mapping of location slugs (used in URLs) to human-readable labels\n",
    "locations = {\n",
    "    \"los-angeles\": \"Los Angeles\",\n",
    "    \"santa-ana\": \"Orange County\",\n",
    "    \"san-diego\": \"San Diego\",\n",
    "    \"riverside\": \"Riverside\",\n",
    "    \"san-bernardino\": \"San Bernardino\",\n",
    "    \"ventura\": \"Ventura\",\n",
    "    \"santa-barbara\": \"Santa Barbara\",\n",
    "    \"el-centro\": \"Imperial\",\n",
    "    \"bakersfield\": \"Kern\",\n",
    "    \"san-luis-obispo\": \"San Luis Obispo\",\n",
    "    \"online\": \"Online\"\n",
    "}\n",
    "\n",
    "# Define the date range to search: from today for the next 30 days\n",
    "days_range = 30\n",
    "start_day = datetime.today().date()\n",
    "\n",
    "# HTTP headers to include a User-Agent to avoid being blocked\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "# Containers for unique event IDs and detailed result entries\n",
    "seen_ids = set()    # Keep track of event IDs we've already processed\n",
    "results = []  # Store dictionaries of event_id, event_date, location\n",
    "\n",
    "# Lists to log any rate-limit or other errors encountered during page fetches\n",
    "rate_limit_errors = []\n",
    "other_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1b651",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Scrape Eventbrite Listing Pages for Event IDs\n",
    "for i in range(days_range):\n",
    "    # Compute the date for this iteration\n",
    "    date = start_day + timedelta(days=i)\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    print(f\"\\nüìÖ Searching for events on {date_str}\")\n",
    "\n",
    "    # Loop through each location and keyword combination\n",
    "    for loc_slug, loc_label in locations.items():\n",
    "        for keyword in keywords:\n",
    "            # Convert spaces to hyphens for URL usage\n",
    "            keyword_slug = keyword.replace(\" \", \"-\")\n",
    "            base_url = f\"https://www.eventbrite.com/d/online/{keyword_slug}/\" if loc_slug == \"online\" \\\n",
    "                       else f\"https://www.eventbrite.com/d/ca--{loc_slug}/{keyword_slug}/\"\n",
    "\n",
    "            # Paginate up to page 50 to collect as many events as possible\n",
    "            for page in range(1, 51):\n",
    "                url = f\"{base_url}?page={page}&start_date={date_str}&end_date={date_str}\"\n",
    "                print(f\"  ‚Üí Fetching page {page}: {loc_label} - {keyword}\")\n",
    "                try:\n",
    "                    # If we hit a rate limit, log and stop paginating this keyword\n",
    "                    resp = requests.get(url, headers=headers, timeout=10)\n",
    "                    if resp.status_code == 429:\n",
    "                        print(f\"    ‚ö†Ô∏è  Rate-limit hit: {url}\")\n",
    "                        rate_limit_errors.append(url)\n",
    "                        break\n",
    "                    elif resp.status_code != 200:\n",
    "                    # For any other non-200 response, log the error and stop paginating\n",
    "                        print(f\"    ‚ùå Error {resp.status_code}: {url}\")\n",
    "                        other_errors.append((resp.status_code, url))\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                # Handle network errors, timeouts, etc.\n",
    "                    print(f\"    ‚ùå Request Exception: {e} - {url}\")\n",
    "                    other_errors.append((\"exception\", url))\n",
    "                    break\n",
    "                \n",
    "                # Parse the HTML to find all links; filter for event URLs\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                anchors = soup.find_all(\"a\", href=True)\n",
    "\n",
    "                new_found = 0 # Count of newly discovered events on this page\n",
    "                for a in anchors:\n",
    "                    href = a[\"href\"]\n",
    "                    if \"/e/\" in href:\n",
    "                        match = EVENT_ID_RE.search(href)\n",
    "\n",
    "                        if match:\n",
    "                            event_id = match.group(1)\n",
    "                            if event_id not in seen_ids:\n",
    "                                seen_ids.add(event_id)\n",
    "                                results.append({\n",
    "                                    \"event_id\": event_id,\n",
    "                                    \"event_date\": date_str,\n",
    "                                    \"location\": loc_label\n",
    "                                })\n",
    "                                new_found += 1\n",
    "                                \n",
    "                # If no new events are found on this page, we can stop paginating\n",
    "                if new_found == 0:\n",
    "                    break\n",
    "                time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4c395",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Process and Display Scrape Results\n",
    "# Convert results to a DataFrame and sort for readability\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values(by=[\"event_date\", \"location\", \"event_id\"], inplace=True)\n",
    "\n",
    "# Calculate total runtime of the scraping phase\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "minutes, seconds = divmod(duration, 60)\n",
    "\n",
    "# Output summary statistics\n",
    "print(\"\\n‚úÖ DONE\")\n",
    "print(f\"üî¢ Total unique event IDs: {len(seen_ids)}\")\n",
    "print(f\"‚è± Total run time: {int(minutes)} minutes {int(seconds)} seconds\")\n",
    "print(f\"‚ö†Ô∏è  Rate-limit hits: {len(rate_limit_errors)}\")\n",
    "if rate_limit_errors:\n",
    "    print(\"    ‚Üí URLs with rate-limit:\")\n",
    "    for url in rate_limit_errors:\n",
    "        print(f\"      - {url}\")\n",
    "print(f\"‚ùå Other errors: {len(other_errors)}\")\n",
    "if other_errors:\n",
    "    print(\"    ‚Üí URLs with other errors:\")\n",
    "    for err, url in other_errors:\n",
    "        print(f\"      - [{err}] {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541662a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Configure Eventbrite API and Output Filenames\n",
    "# Base URL for the Eventbrite API\n",
    "API_BASE   = \"https://www.eventbriteapi.com/v3\"\n",
    "\n",
    "# Filenames for saving output\n",
    "OUTPUT_CSV = \"combine2.csv\"\n",
    "OUTPUT_MD  = \"combine2.md\"\n",
    "\n",
    "# List of OAuth tokens for API access; rotate through these to handle rate limits\n",
    "TOKENS = [\n",
    "    \"ADFEE3M6QKP4UHHK4RTJ\",\"AUUSEMT6BFTDLYNMDFPU\",\"7Z7AWALOH2TLA7XQX7OK\",\n",
    "    \"YGRRGPXV7F7NQV4HH5RU\",\"4H774ER6HNVINEL627TZ\",\"TGIPUCHUYCVBACWGG6GI\",\n",
    "    \"GO62D54NYILEA3AFNH4J\",\"6MVAAMPKWFWNRPXGNODG\",\"BQQLKRU5JPAGIOOJUGVH\",\n",
    "    \"LYUIAO4THHDCKM3SNJBE\",\"5FEMNH4LDL4RQQEKEAL4\",\"XT6F7O5DVONZOG7WOTKX\",\n",
    "    \"QAOTJ65YVJLDE3RHABAF\",\"CPXY6BZLSP2WCLQ3ZQIS\",\"OOAWYP6D7KIN25F5M6PZ\",\n",
    "    \"6AG6WFR46S7SKYU5GY7K\",\"ZA54YJ7YCHEMUBBV7UA6\",\"HVHSEU223FIKILV4D3T5\",\n",
    "    \"237RK6M5K54Z7DD5XIHW\",\"HXGETSGAZDES2B5ZNNWF\",\"AQG3T27GPOQVDLUCPU5J\",\n",
    "    \"6JUXTETTGHTPR2FUPXOW\",\"KUMPCT5DZILKWCRRRPVP\",\"JMXYZ5ANBW57IO6CBQ7A\",\n",
    "    \"7BXG7G6JGBP7UP43PZAP\",\"G6UAN755SVY3VBOCGL6Y\",\"QMMBVEKVMYYQ3ZDA6HQK\",\n",
    "    \"INTZNCHDKQWHMPZPX4BU\",\"HNF6UNGUAUQ77C76HNVL\",\"ZHLGC7OBFFPRZ6PZRX5V\",\n",
    "    \"RFW5X5UYFUQSOZ2OJMAZ\",\"TSZG4QRXXOA4WYKPULGI\",\"CQOOLXW3DUDCWDCTTFLL\",\n",
    "    \"M3CU2VLQ2OSO2MDIVLXC\",\"O3EULVSK6NWIJJ4XSST3\",\"75WESU4HDVHHRQ4IXIYZ\",\n",
    "    \"7JWMEX6CPI4RKJYRJS6W\",\"MWCOODQPBRCLX6L5GWCV\",\"22UAT7BSC3MQQSF2VJRD\",\n",
    "    \"NRCVYNKLDIM7KHJKNJHL\",\"FVFJFHCXJG6B7EJ6DN26\",\"K32AEV7RONXA7QV5BXRS\",\n",
    "    \"YIFSVQXG3AYAYCUDTOSC\",\"6HREUV3Q35K2CKRM5CJQ\",\"MGJFNF7SRKG3MKHDU6G4\",\n",
    "    \"SATAELGXQVFICGZ6PXXH\",\"IF3SIR6NVOLQMUJI455K\",\"E524YMHHMNORUOTNXSKS\",\n",
    "    \"ECJYLU4WTF3QGPKOHEHS\",\"C3PYJRSQT7DCWRJUGJBK\",\"IZDL5NYVJUOM3DZKMIV7\",\n",
    "    \"6YD6RT24JFGEHBNGUOD6\",\"AO5GIQCLCERRTOU4CPTU\",\"7VYNZF74DKHTOZZRLTXM\",\n",
    "    \"M3G6KP4UXZ5PFGVNMNZZ\",\"BOSYHVEDOHQJN3SU742M\",\"ICYWMBRWIO726IU5FGPI\",\n",
    "    \"PIBEZGMWDR77L2T3UAQZ\",\"RZQC46TK6TVQCFV2FTED\",\"TL4MCD5NJBY5X7S7GRTX\",\n",
    "    \"UGQ3E3E63KRL4CLWQTXH\",\"NYMXR3VLCNQO7Q7CIZAI\",\"Y5RIULQGUAKLZGXIFTNW\",\n",
    "    \"PKIQ7Z4QFO6XKSED3G2P\",\"FBNGOQ6T7WJ7IS7ROGAN\",\"QVNLNFZTKR5JA2MWTGXZ\",\n",
    "    \"NVUE2KM2JSSEYTSESDM6\",\"OJFN4TBC3J3LPSV6AP6M\"\n",
    "]\n",
    "\n",
    "# Ensure at least one token is provided\n",
    "if not TOKENS:\n",
    "    raise RuntimeError(\"Please populate the TOKENS list with at least one OAuth token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7e6dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Define Helper Functions for API Data Extraction\n",
    "def fetch_event(session, event_id):\n",
    "    \"\"\"\n",
    "    Fetch detailed event data by ID, expanding organizer, venue, and ticket info.\n",
    "    Returns a tuple (json_data, error_type, error_info).\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/events/{event_id}/\"\n",
    "    params = {\"expand\": \"organizer,venue,ticket_classes\"}\n",
    "    try:\n",
    "        resp = session.get(url, params=params)\n",
    "    except Exception as e:\n",
    "        return None, 'exception', str(e)\n",
    "    if resp.status_code == 429:\n",
    "        return None, 'rate_limit', None\n",
    "    if not resp.ok:\n",
    "        return None, 'http_error', f\"status {resp.status_code}\"\n",
    "    try:\n",
    "        return resp.json(), None, None\n",
    "    except ValueError as e:\n",
    "        return None, 'parse_error', str(e)\n",
    "\n",
    "\n",
    "def format_address(venue):\n",
    "    \"\"\"\n",
    "    Format the venue address; return 'Online' for online events or 'Unknown' if missing.\n",
    "    \"\"\"\n",
    "    if not venue or venue.get(\"online_event\"):\n",
    "        return \"Online\" if venue and venue.get(\"online_event\") else \"Unknown\"\n",
    "    addr = venue.get(\"address\", {}) or {}\n",
    "    parts = [\n",
    "        addr.get(\"address_1\", \"\"), addr.get(\"address_2\", \"\"),\n",
    "        addr.get(\"city\", \"\"), addr.get(\"region\", \"\"),\n",
    "        addr.get(\"postal_code\", \"\"), addr.get(\"country\", \"\")\n",
    "    ]\n",
    "    return \", \".join(p.strip() for p in parts if p and p.strip()) or \"Unknown\"\n",
    "\n",
    "\n",
    "def extract_fee(ticket_classes):\n",
    "    \"\"\"\n",
    "    Extract the first available ticket fee or return 'free', 'sold out', or 'Unknown'.\n",
    "    \"\"\"\n",
    "    fees = []\n",
    "    for tc in (ticket_classes or []):\n",
    "        if tc.get(\"is_sold_out\"):\n",
    "            fees.append(\"sold out\")\n",
    "        else:\n",
    "            cost = tc.get(\"cost\")\n",
    "            if cost:\n",
    "                val = cost.get(\"value\", 0) / 100\n",
    "                cur = cost.get(\"currency\", \"\").upper()\n",
    "                fees.append(f\"{cur} {val:.2f}\")\n",
    "            elif tc.get(\"free\"):\n",
    "                fees.append(\"free\")\n",
    "    return fees[0] if fees else \"Unknown\"\n",
    "\n",
    "\n",
    "def to_pt(utc_str):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamp string to Pacific Time and return a datetime.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(utc_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return pytz.utc.localize(dt).astimezone(pytz.timezone(\"America/Los_Angeles\"))\n",
    "\n",
    "\n",
    "def clean_desc(text):\n",
    "    \"\"\"\n",
    "    Clean up description text by collapsing whitespace.\n",
    "    \"\"\"\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eadd18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Multi-threaded Fetch of Event Details\n",
    "# Start timing the API fetch phase\n",
    "run_start = time.time()\n",
    "now_utc = datetime.now(pytz.utc)\n",
    "cutoff  = now_utc + timedelta(days=30)\n",
    "\n",
    "event_ids = list(seen_ids)\n",
    "total_calls = len(event_ids)\n",
    "\n",
    "# Populate a thread-safe queue with event IDs\n",
    "q = queue.Queue()\n",
    "for eid in event_ids:\n",
    "    q.put(eid)\n",
    "\n",
    "# Prepare structures to collect records and track failures\n",
    "records = []\n",
    "failure_counts = {'rate_limit': 0, 'http_error': 0, 'exception': 0, 'parse_error': 0}\n",
    "failure_details = []\n",
    "stats = {'success': 0}\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Worker function to fetch one event at a time\n",
    "def worker(token):\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    })\n",
    "    while True:\n",
    "        try:\n",
    "            eid = q.get_nowait()\n",
    "        except queue.Empty:\n",
    "            return\n",
    "        data, err_type, err_info = fetch_event(session, eid)\n",
    "        time.sleep(3)\n",
    "\n",
    "        with lock:\n",
    "            if err_type:\n",
    "                failure_counts[err_type] += 1\n",
    "                failure_details.append((eid, err_type, err_info))\n",
    "            else:\n",
    "                stats['success'] += 1\n",
    "                start_utc = data.get(\"start\", {}).get(\"utc\")\n",
    "                if not start_utc:\n",
    "                    failure_counts['parse_error'] += 1\n",
    "                    failure_details.append((eid, 'parse_error', 'missing start.utc'))\n",
    "                else:\n",
    "                    dt_utc = pytz.utc.localize(datetime.strptime(start_utc, \"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "                    if now_utc <= dt_utc <= cutoff:\n",
    "                        name_obj  = data.get(\"name\") or {}\n",
    "                        desc_obj  = data.get(\"description\") or {}\n",
    "                        org_obj   = data.get(\"organizer\") or {}\n",
    "                        venue_obj = data.get(\"venue\")\n",
    "                        online_flag = data.get(\"online_event\", False)\n",
    "\n",
    "                        title     = (name_obj.get(\"text\") or \"\").strip() or \"No title\"\n",
    "                        organizer = (org_obj.get(\"name\") or \"\").strip() or \"Unknown organizer\"\n",
    "                        raw_desc  = (desc_obj.get(\"text\") or \"\")\n",
    "                        desc      = clean_desc(raw_desc) or \"No description provided\"\n",
    "                        fee       = extract_fee(data.get(\"ticket_classes\"))\n",
    "                        address   = \"Online\" if online_flag else format_address(venue_obj)\n",
    "                        url_event = data.get(\"url\", \"\").strip() or \"No URL\"\n",
    "\n",
    "                        records.append({\n",
    "                            \"Event ID\": eid,\n",
    "                            \"Title\": title,\n",
    "                            \"Organizer\": organizer,\n",
    "                            \"Start (PT)\": to_pt(start_utc).strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                            \"Address\": address,\n",
    "                            \"Description\": desc,\n",
    "                            \"URL\": url_event,\n",
    "                            \"Fee\": fee\n",
    "                        })\n",
    "        q.task_done()\n",
    "\n",
    "# Launch threads for each token\n",
    "threads = [threading.Thread(target=worker, args=(tkn,), daemon=True) for tkn in TOKENS]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "# Sort records by start time\n",
    "records.sort(key=lambda r: r[\"Start (PT)\"])\n",
    "\n",
    "# Save to CSV\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csvf:\n",
    "    writer = csv.writer(csvf)\n",
    "    writer.writerow([\"Event ID\",\"Title\",\"Organizer\",\"Start (PT)\",\"Address\",\"Description\",\"URL\",\"Fee\"])\n",
    "    for r in records:\n",
    "        writer.writerow([r[k] for k in [\"Event ID\",\"Title\",\"Organizer\",\"Start (PT)\",\"Address\",\"Description\",\"URL\",\"Fee\"]])\n",
    "\n",
    "# Save to Markdown\n",
    "with open(OUTPUT_MD, \"w\", encoding=\"utf-8\") as md:\n",
    "    md.write(\"# Upcoming Career & Leadership Events (Next 30 Days, SoCal)\\n\\n\")\n",
    "    for r in records:\n",
    "        md.write(f\"- **{r['Title']}**\\n\")\n",
    "        md.write(f\"  - Organizer: {r['Organizer']}\\n\")\n",
    "        md.write(f\"  - When: {r['Start (PT)']}\\n\")\n",
    "        md.write(f\"  - Address: {r['Address']}\\n\")\n",
    "        md.write(f\"  - Description: {r['Description']}\\n\")\n",
    "        md.write(f\"  - URL: {r['URL']}\\n\")\n",
    "        md.write(f\"  - Fee: {r['Fee']}\\n\\n\")\n",
    "\n",
    "# Compute and print run statistics\n",
    "elapsed = time.time() - run_start\n",
    "total_failures = sum(failure_counts.values())\n",
    "\n",
    "print(\"\\n=== Run Statistics ===\")\n",
    "print(f\"Total API calls attempted: {total_calls}\")\n",
    "print(f\"  Successful fetches:      {stats['success']}\")\n",
    "for etype, cnt in failure_counts.items():\n",
    "    print(f\"  {etype.replace('_',' ').title():<22}: {cnt}\")\n",
    "print(f\"Total failures:            {total_failures}\")\n",
    "if failure_details:\n",
    "    print(\"\\nFailure details (Event ID, Type, Info):\")\n",
    "    for fid, ftype, finfo in failure_details:\n",
    "        info = finfo or \"(no additional info)\"\n",
    "        print(f\"  - {fid}: {ftype} ‚Äî {info}\")\n",
    "print(f\"\\nTotal runtime: {elapsed:.2f} seconds\")\n",
    "print(f\"Wrote CSV ‚Üí {OUTPUT_CSV}\")\n",
    "print(f\"Wrote Markdown ‚Üí {OUTPUT_MD}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
